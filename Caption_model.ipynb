{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e09e2f-c902-410d-bada-2c1c4cf27fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (0.16.0)\n",
      "Requirement already satisfied: torch in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.26.1)\n",
      "Requirement already satisfied: requests in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: filelock in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b65473e-cab8-4215-b23d-1477b80b64bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9a9539-4fe3-47cf-8533-22c1cda25b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4701ba-2276-4b7c-9c64-8d58491127c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade urllib3\n",
    "# !pip install --upgrade accelerate\n",
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d20db9-d5fe-4938-9d60-ecd60bc7eed5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io, transforms\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader, random_split\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqTrainer ,Seq2SeqTrainingArguments\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionEncoderDecoderModel , ViTFeatureExtractor\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer ,  GPT2Config , default_data_collator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import io, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# from transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\n",
    "# from transformers import VisionEncoderDecoderModel , ViTFeatureExtractor\n",
    "# from transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import clip\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa34720-0e5f-49ea-bef5-724668cbd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config: \n",
    "    ENCODER = \"google/vit-base-patch16-224\" # ruclip-vit-base-patch32-384\n",
    "    DECODER = \"gpt2\" # rugpt3large_based_on_gpt2\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VAL_BATCH_SIZE = 8\n",
    "    VAL_EPOCHS = 1\n",
    "    LR = 5e-5\n",
    "    SEED = 42\n",
    "    MAX_LEN = 128\n",
    "    SUMMARY_LEN = 20\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MEAN = (0.485, 0.456, 0.406)\n",
    "    STD = (0.229, 0.224, 0.225)\n",
    "    TRAIN_PCT = 0.95\n",
    "    NUM_WORKERS = mp.cpu_count()\n",
    "    EPOCHS = 1\n",
    "    IMG_SIZE = (224, 224)\n",
    "    LABEL_MASK = -100\n",
    "    TOP_K = 1000\n",
    "    TOP_P = 0.95\n",
    "    PREFIX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec8726-175d-40d6-8302-c93be9473a58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6964a0-1663-441a-b35f-a2239b2a1135",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60002d-739b-484b-abd8-fd5ed9c60958",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "images_url = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
    "annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    " \n",
    "download_folder = \"coco_data\"\n",
    "\n",
    "# Создаем папку, если ее нет\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# Скачивание и распаковка изображений\n",
    "images_zip_path = os.path.join(download_folder, \"train2017.zip\")\n",
    "annotations_zip_path = os.path.join(download_folder, \"annotations_trainval2017.zip\")\n",
    "\n",
    "# Скачивание изображений\n",
    "with requests.get(images_url, stream=True) as r:\n",
    "    with open(images_zip_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "# Скачивание аннотаций\n",
    "with requests.get(annotations_url, stream=True) as r:\n",
    "    with open(annotations_zip_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "# Распаковка архивов\n",
    "with zipfile.ZipFile(images_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(download_folder)\n",
    "\n",
    "with zipfile.ZipFile(annotations_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(download_folder)\n",
    "\n",
    "# Удаление загруженных zip-архивов (если нужно)\n",
    "os.remove(images_zip_path)\n",
    "os.remove(annotations_zip_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c00ade5c-d3ef-4aa2-adf4-7b560d4b35f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118287\n",
      "000000391895.jpg\n",
      "A bicycle replica with a clock as the front wheel.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Путь к файлу с аннотациями\n",
    "json_file_path = 'coco_data/annotations/captions_train2017.json'\n",
    "\n",
    "# Путь к текстовому файлу, в который будем записывать данные\n",
    "output_txt_path = 'coco_data/annotations/captions_train2017.txt'\n",
    "\n",
    "# Чтение JSON-файла\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(len(data['images']))\n",
    "    print(data['images'][0]['file_name'])\n",
    "    print(data['annotations'][0]['caption'])\n",
    "\n",
    "# Открытие текстового файла для записи\n",
    "# with open(output_txt_path, 'w') as output_txt_file:\n",
    "#     # Обход каждого элемента в JSON-файле\n",
    "#     for annotation in data['annotations']:\n",
    "#         # Получение пути к файлу изображения и аннотации\n",
    "#         image_path = annotation['file_name']\n",
    "#         caption = annotation['caption']\n",
    "        \n",
    "#         # Запись в текстовый файл с разделением табуляцией\n",
    "#         output_txt_file.write(f\"{image_path}\\t{caption}\\n\")\n",
    "\n",
    "# print(f\"Annotations extracted and written to {output_txt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d74e91-14b8-44e9-aa12-2fee377ca9d1",
   "metadata": {},
   "source": [
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "42a46737-5631-4b75-b9c9-958e13af6a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118287, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000391895.jpg</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000522418.jpg</td>\n",
       "      <td>A room with blue walls and a white sink and door.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000184613.jpg</td>\n",
       "      <td>A car that seems to be parked illegally behind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000318219.jpg</td>\n",
       "      <td>A large passenger airplane flying through the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000554625.jpg</td>\n",
       "      <td>There is a GOL plane taking off in a partly cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_path                                            caption\n",
       "0  000000391895.jpg  A bicycle replica with a clock as the front wh...\n",
       "1  000000522418.jpg  A room with blue walls and a white sink and door.\n",
       "2  000000184613.jpg  A car that seems to be parked illegally behind...\n",
       "3  000000318219.jpg  A large passenger airplane flying through the ...\n",
       "4  000000554625.jpg  There is a GOL plane taking off in a partly cl..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Путь к файлу с аннотациями\n",
    "json_file_path = 'coco_data/annotations/captions_train2017.json'\n",
    "\n",
    "# Чтение JSON-файла\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Создание списка для данных\n",
    "annotations_data = {'image_path': [],\n",
    "                   'caption': []}\n",
    "\n",
    "# Обход каждой аннотации в JSON-файле\n",
    "for index in range(0, 118287):\n",
    "    annotations_data['image_path'].append(data['images'][index]['file_name'])\n",
    "    annotations_data['caption'].append(data['annotations'][index]['caption'])\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(annotations_data)\n",
    "\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9eaee5aa-ebb6-4f9a-a513-8c7dc8a01180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000397133.jpg</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000037777.jpg</td>\n",
       "      <td>A Honda motorcycle parked in a grass driveway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000252219.jpg</td>\n",
       "      <td>An office cubicle with four different types of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000087038.jpg</td>\n",
       "      <td>A small closed toilet in a cramped space.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000174482.jpg</td>\n",
       "      <td>Two women waiting at a bench next to a street.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_path                                            caption\n",
       "0  000000397133.jpg  A black Honda motorcycle parked in front of a ...\n",
       "1  000000037777.jpg      A Honda motorcycle parked in a grass driveway\n",
       "2  000000252219.jpg  An office cubicle with four different types of...\n",
       "3  000000087038.jpg          A small closed toilet in a cramped space.\n",
       "4  000000174482.jpg     Two women waiting at a bench next to a street."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Путь к файлу с аннотациями\n",
    "json_file_path = 'coco_data/annotations/captions_val2017.json'\n",
    "\n",
    "# Чтение JSON-файла\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Создание списка для данных\n",
    "annotations_data = {'image_path': [],\n",
    "                   'caption': []}\n",
    "\n",
    "# Обход каждой аннотации в JSON-файле\n",
    "for index in range(0, 5000):\n",
    "    annotations_data['image_path'].append(data['images'][index]['file_name'])\n",
    "    annotations_data['caption'].append(data['annotations'][index]['caption'])\n",
    "\n",
    "\n",
    "df_val = pd.DataFrame(annotations_data)\n",
    "\n",
    "print(df_val.shape)\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a72b6-6164-4d6c-8fbe-7bb3eec6b2da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ImgDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f200e04a-c102-40f6-accb-db43d50c4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df , val_df = train_test_split(df_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f075633d-e4bb-4db1-b5fa-a64e8b1de587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4877a9200e6243109173f0569dbc7037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)24/resolve/main/preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b20217b2fd042cb86537cf4895fa49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ingface.co/gpt2/resolve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b660a10ee6f949e39530bbdf5631d1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)gingface.co/gpt2/resolve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb771aa097046a59d89289d2d3bc9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)gingface.co/gpt2/resolve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a12e80633647319be1175c9de7704c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)face.co/gpt2/resolve/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(config.ENCODER)\n",
    "tokenizer = GPT2LMHeadModel.from_pretrained(config.DECODER)\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "09d65152-76d8-4b38-ba4f-ea792f76a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(config.IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(\n",
    "        # mean=0.5,\n",
    "        # std=0.5,\n",
    "        # ),\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0842b32b-65aa-4da0-b909-3aa5dfe3d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(df_train['caption'].iloc[5], padding='max_length', max_length=50).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2ffd1efe-36b2-4360-940d-79d3c9d6dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train['caption'].iloc[0])\n",
    "# print(df_train['image_path'].iloc[0])\n",
    "# transforms(Image.open(df_train['image_path'].iloc[4]).convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "10f0899c-d6c9-4f7f-8ebf-4032662cb8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, tokenizer, feature_extractor, transform = None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer= tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = 50\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.df['caption'].iloc[idx]\n",
    "        image = self.df['image_path'].iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, image)\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # print(img)\n",
    "        pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values\n",
    "        captions = self.tokenizer(caption,\n",
    "                                  padding='max_length',\n",
    "                                  max_length=self.max_length).input_ids\n",
    "        \n",
    "        tokens = torch.tensor(captions)\n",
    "        prefix = pixel_values\n",
    "        return tokens, prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c04fdbdc-f0b6-4b59-b9cc-c8ecf6d16adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImgDataset(train_df, root_dir = \"coco_data/train2017\", tokenizer=tokenizer, feature_extractor = feature_extractor, transform = transforms)\n",
    "val_dataset = ImgDataset(val_df, root_dir = \"coco_data/train2017\", tokenizer=tokenizer, feature_extractor = feature_extractor, transform  = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "10e1b5ec-be6d-4d1b-b575-fa2173fb3fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  464,  9283,  2137, 26728,   465,  7365,   379,   281, 13885,  9283,\n",
       "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n",
       " tensor([[[[-0.9942, -0.9942, -0.9942,  ..., -0.9938, -0.9937, -0.9935],\n",
       "           [-0.9942, -0.9942, -0.9942,  ..., -0.9937, -0.9936, -0.9934],\n",
       "           [-0.9942, -0.9942, -0.9942,  ..., -0.9937, -0.9936, -0.9934],\n",
       "           ...,\n",
       "           [-0.9988, -0.9988, -0.9988,  ..., -0.9930, -0.9932, -0.9934],\n",
       "           [-0.9988, -0.9988, -0.9988,  ..., -0.9930, -0.9931, -0.9933],\n",
       "           [-0.9988, -0.9988, -0.9988,  ..., -0.9930, -0.9930, -0.9932]],\n",
       " \n",
       "          [[-0.9942, -0.9942, -0.9942,  ..., -0.9938, -0.9938, -0.9938],\n",
       "           [-0.9942, -0.9942, -0.9942,  ..., -0.9938, -0.9937, -0.9937],\n",
       "           [-0.9942, -0.9942, -0.9942,  ..., -0.9938, -0.9937, -0.9938],\n",
       "           ...,\n",
       "           [-0.9987, -0.9987, -0.9987,  ..., -0.9930, -0.9931, -0.9933],\n",
       "           [-0.9987, -0.9987, -0.9987,  ..., -0.9930, -0.9930, -0.9932],\n",
       "           [-0.9985, -0.9987, -0.9987,  ..., -0.9930, -0.9930, -0.9931]],\n",
       " \n",
       "          [[-0.9944, -0.9943, -0.9943,  ..., -0.9939, -0.9939, -0.9942],\n",
       "           [-0.9944, -0.9943, -0.9943,  ..., -0.9939, -0.9939, -0.9946],\n",
       "           [-0.9944, -0.9943, -0.9943,  ..., -0.9939, -0.9942, -0.9950],\n",
       "           ...,\n",
       "           [-0.9986, -0.9986, -0.9986,  ..., -0.9929, -0.9930, -0.9932],\n",
       "           [-0.9986, -0.9986, -0.9986,  ..., -0.9929, -0.9929, -0.9931],\n",
       "           [-0.9984, -0.9986, -0.9986,  ..., -0.9929, -0.9929, -0.9930]]]]))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09480b3a-f66e-4692-8335-c2b7eed43cbe",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcf25e7-b607-4527-a543-1d5328a973f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08721931-1cc5-4eb0-aa6b-db02518878b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Using cached pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting install\n",
      "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
      "Requirement already satisfied: transformers in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (4.35.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: filelock in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: requests in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: fsspec in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3.0)\n",
      "Installing collected packages: pip, install\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.2\n",
      "    Uninstalling pip-22.2.2:\n",
      "      Successfully uninstalled pip-22.2.2\n",
      "Successfully installed install-1.3.5 pip-23.3.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c868c5-d8cd-4b48-8534-d1d2dc8089da",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.peft because of the following error (look up to see its traceback):\ncannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/integrations/peft.py:29\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_model\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_balanced_memory, infer_auto_device_map\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.24.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     cpu_offload,\n\u001b[1;32m      6\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/accelerator.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/checkpointing.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MODEL_NAME,\n\u001b[1;32m     26\u001b[0m     OPTIMIZER_NAME,\n\u001b[1;32m     27\u001b[0m     RNG_STATE_NAME,\n\u001b[1;32m     28\u001b[0m     SAMPLER_NAME,\n\u001b[1;32m     29\u001b[0m     SCALER_NAME,\n\u001b[1;32m     30\u001b[0m     SCHEDULER_NAME,\n\u001b[1;32m     31\u001b[0m     get_pretty_name,\n\u001b[1;32m     32\u001b[0m     is_tpu_available,\n\u001b[1;32m     33\u001b[0m     is_xpu_available,\n\u001b[1;32m     34\u001b[0m     save,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/utils/__init__.py:139\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    140\u001b[0m     PrepareForLaunch,\n\u001b[1;32m    141\u001b[0m     _filter_args,\n\u001b[1;32m    142\u001b[0m     prepare_deepspeed_cmd_env,\n\u001b[1;32m    143\u001b[0m     prepare_multi_gpu_env,\n\u001b[1;32m    144\u001b[0m     prepare_sagemager_args_inputs,\n\u001b[1;32m    145\u001b[0m     prepare_simple_launcher_cmd_env,\n\u001b[1;32m    146\u001b[0m     prepare_tpu,\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmegatron_lm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    149\u001b[0m     AbstractTrainStep,\n\u001b[1;32m    150\u001b[0m     BertTrainStep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     gather_across_data_parallel_groups,\n\u001b[1;32m    160\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/utils/launch.py:32\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEEPSPEED_MULTINODE_LAUNCHERS\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mother\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_port_in_use, merge_dicts\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedType, SageMakerDistributedType\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/utils/other.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommands\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m write_basic_config  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/commands/config/__init__.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_command_parser\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_config_file, load_config_from_file  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/commands/config/config.py:25\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ask_field, _ask_options, _convert_compute_environment  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msagemaker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_sagemaker_input\n\u001b[1;32m     28\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunches a series of prompts to create and save a `default_config.yaml` configuration file for your training system. Should always be ran first on your machine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/accelerate/commands/config/sagemaker.py:35\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_boto3_available():\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_iam_role_for_sagemaker\u001b[39m(role_name):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/boto3/__init__.py:17\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _warn_deprecated_python\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Session\n\u001b[1;32m     19\u001b[0m __author__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmazon Web Services\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/boto3/session.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/session.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigloader\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/client.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m waiter, xform_name\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientArgsCreator\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/waiter.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjmespath\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WaiterDocstring\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_service_module_name\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/docs/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceDocumenter\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_docs\u001b[39m(root_dir, session):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/docs/service.py:14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbcdoc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrestdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentStructure\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientDocumenter, ClientExceptionsDocumenter\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaginator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaginatorDocumenter\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/docs/client.py:14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResponseExampleDocumenter\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethod\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     document_custom_method,\n\u001b[1;32m     17\u001b[0m     document_model_driven_method,\n\u001b[1;32m     18\u001b[0m     get_instance_public_methods,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/docs/example.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeDocumenter\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_default\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/docs/shape.py:19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# inherited from a Documenter class with the appropriate methods\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# and attributes.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_json_value_header\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mShapeDocumenter\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/utils.py:34\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mawsrequest\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttpsession\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# IP Regexes retained for backwards compatibility\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/botocore/httpsession.py:21\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Retry\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mssl_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     DEFAULT_CIPHERS,\n\u001b[1;32m     23\u001b[0m     OP_NO_COMPRESSION,\n\u001b[1;32m     24\u001b[0m     PROTOCOL_TLS,\n\u001b[1;32m     25\u001b[0m     OP_NO_SSLv2,\n\u001b[1;32m     26\u001b[0m     OP_NO_SSLv3,\n\u001b[1;32m     27\u001b[0m     is_ipaddress,\n\u001b[1;32m     28\u001b[0m     ssl,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01murl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_url\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:38\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m     33\u001b[0m     CausalLMOutputWithCrossAttentions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     TokenClassifierOutput,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, SequenceSummary\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/modeling_utils.py:42\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig, GenerationMixin\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     Conv1D,\n\u001b[1;32m     45\u001b[0m     apply_chunking_to_forward,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     prune_linear_layer,\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1335\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1335\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1336\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1348\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1350\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.peft because of the following error (look up to see its traceback):\ncannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2LMHeadModel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMLP\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1336\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1335\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1336\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1335\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1335\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1336\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1348\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1350\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.gpt2.modeling_gpt2 because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.peft because of the following error (look up to see its traceback):\ncannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py)"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    @autocast()  \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    def __init__(self, backbone, prefix_length: int, prefix_size: int = 24):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(backbone)\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
    "                                self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "    \n",
    "    @autocast() \n",
    "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None):\n",
    "\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        print(\"Prefix projections shape:\", prefix_projections.shape)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "  \n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be9c85-9d5b-4380-adfc-c8c5c5e19edb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54dfe4f-9cb2-48b2-95fc-f0311677756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption(prefix, model, device, tokenizer, prompt=''):\n",
    "    prefix = prefix.to(device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        prefix_embed = model.clip_project(prefix).reshape(len(prefix), config.PREFIX_LENGHT, -1)\n",
    "\n",
    "        answers = []\n",
    "\n",
    "        for x in range(len(prefix_embed)):\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            cur_prefix_embed = prefix_embed[x].unsqueeze(0).to('cpu')\n",
    "            \n",
    "            print('first', time.time()-start)\n",
    "            \n",
    "            if prompt:\n",
    "                generated_text_prefix = generate2(model, tokenizer, prompt=prompt, embed=cur_prefix_embed)\n",
    "            else:\n",
    "                generated_text_prefix = generate2(model, tokenizer, embed=cur_prefix_embed)\n",
    "            \n",
    "            print('second', time.time()-start)\n",
    "        \n",
    "            answers.append(generated_text_prefix.replace('\\n',' ').replace('<|endoftext|',''))\n",
    "\n",
    "    return [x[len(prompt):].strip() for x in answers]\n",
    "\n",
    "# def get_ans(model, clip_emb, prompt, device, tokenizer):\n",
    "#     output = get_caption(clip_emb, model, device, tokenizer, prompt=prompt)\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72bbe9ce-a418-43a4-a842-95dbdc5ee74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, scheduler, device, epoch):\n",
    "    loss_avg = AverageMeter()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    progress = tqdm(total=len(train_loader))\n",
    "    for idx, (tokens, prefix) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        tokens, prefix = tokens.to(device), prefix.to(device, dtype=torch.float32)\n",
    "        \n",
    "        outputs = model(tokens, prefix)\n",
    "        logits = outputs.logits[:, config.PREFIX_LENGHT-1: -1]\n",
    "\n",
    "        loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
    "\n",
    "        segments = 2\n",
    "\n",
    "        # out = checkpoint_sequential(modules, segments, input_var)\n",
    "\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        clipping_value = 0.5 # arbitrary value of your choosing\n",
    "        #torch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n",
    "\n",
    "        loss_avg.update(loss.item(), len(mask))\n",
    "        progress.set_description(f\"loss: {loss_avg.avg:.5f}\")\n",
    "        progress.update()\n",
    "        \n",
    "\n",
    "        # del tokens\n",
    "        # del mask\n",
    "        # del prefix\n",
    "        torch.clear_autocast_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "    progress.close()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def valid(model, valid_loader, device, gt):\n",
    "    loss_avg = AverageMeter()\n",
    "    model.eval()\n",
    "    progress = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(CFG.backbone)\n",
    "    \n",
    "\n",
    "    all_answers = []\n",
    "    for idx, (tokens, prefix) in progress:\n",
    "        tokens, prefix = tokens.to(device), prefix.to(device, dtype=torch.float32)\n",
    "        answer = get_caption(prefix, model, device, tokenizer, prompt='Caption: ')\n",
    "        all_answers.append(answer)\n",
    "    score = bleu_metric(gt, np.concatenate(all_answers))\n",
    "    return score\n",
    "    \n",
    "\n",
    "import nltk\n",
    "def bleu_metric(ground_truth, prediction):\n",
    "    scores = []\n",
    "    for gt, pred in zip(ground_truth, prediction):\n",
    "        if type(pred)==str and type(gt)==str:\n",
    "            score = nltk.translate.bleu_score.sentence_bleu([gt.lower().split()], pred.lower().replace('<|endoftext|>','').split(), weights = (0.5, 0.5))\n",
    "        scores+=[score]\n",
    "    return np.array(scores).mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c31b1-56e7-4102-a9c9-618dcb9af37c",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec6da6b-00c1-4b5a-a00b-03b20b67e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bitsandbytes as bnb\n",
    "import gc\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "import torchvision\n",
    "import transformers\n",
    "import torch\n",
    "from torch.nn import functional as nnf\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "992c7cb7-2d37-436c-9b55-1f531615b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb9ea4f-f2cc-486d-bff9-eee206f92be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    loss_avg = AverageMeter()\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=config.VAL_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = ClipCaptionModel(prefix_length = config.PREFIX_LENGTH, backbone = config.DECODER)\n",
    "\n",
    "    # model.load_state_dict(torch.load('/content/drive/MyDrive/Olimpiads/nto_hack_2022/V2/v2_1.pt', map_location='cpu'))\n",
    "    model = model.to(device)\n",
    "   \n",
    "    #model = freeze(model)\n",
    "\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=0.33, betas=(0.9, 0.995))\n",
    "    #optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n",
    "    #optimizer = SM3(model.parameters(),lr=args.lr)\n",
    "    #Adafactor(model.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=3, num_training_steps=config.EPOCHS * len(train_loader))\n",
    "    #AdafactorSchedule(optimizer)#num_training_steps=epochs * len(train_loader)\n",
    "\n",
    "    for epoch in range(1, 1+config.EPOCHS):\n",
    "        train(train_loader, model, optimizer, scheduler, device, epoch)\n",
    "        valid(model, valid_loader, device, valid_df.caption.tolist()) \n",
    "\n",
    "        if epoch % 1 ==0:\n",
    "            torch.save(model.state_dict(),os.path.join(f\"model_test.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8521c65f-977b-47ab-9d10-91a0c57c998c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     loss_avg \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[0;32m----> 4\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mTRAIN_BATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mVAL_BATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m ClipCaptionModel(prefix_length \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mPREFIX_LENGTH, backbone \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mDECODER)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac16a16-791b-4c1c-bb9c-21ee5f9a13d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01aba52-6d06-4d57-992d-c037bdd5d83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "85246cd5-1104-4b61-8417-0dab29442587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f0d48f2854430dbc9e5026c8e182ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ase-patch16-224/resolve/main/config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6da1182d2a4a0d996b884eba1e77c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81de4777785442ddb16427edc396a6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.9.ln_cross_attn.weight', 'h.11.ln_cross_attn.weight', 'h.9.ln_cross_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.5.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.7.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.3.crossattention.c_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.11.ln_cross_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.bias', 'h.4.ln_cross_attn.bias', 'h.8.ln_cross_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.1.ln_cross_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.10.crossattention.c_attn.bias', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.1.ln_cross_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.bias', 'h.10.ln_cross_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.8.ln_cross_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.7.ln_cross_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.1.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.6.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.weight', 'h.5.ln_cross_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.5.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.10.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.bias', 'h.4.ln_cross_attn.weight', 'h.3.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.2.ln_cross_attn.bias', 'h.3.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.3.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.8.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e9cd4065ac4015ad1b922f5365e031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)gpt2/resolve/main/generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(config.ENCODER, config.DECODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "da3aafeb-e37c-4053-aa82-5eff8bc78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.max_length = 128\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d33cc-7260-48b6-aea1-3f7f6e3f81d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd2f43-57cf-4dea-ad24-eb5a2a90418a",
   "metadata": {},
   "source": [
    "## Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e0facd54-a820-401c-b439-8f8c5a155b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2ca00bf1-5a79-4eb3-9e3e-d2b510f609e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m615.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: psutil in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: huggingface-hub in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: networkx in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: fsspec in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: jinja2 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: filelock in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: sympy in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: requests in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kekgerman/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.24.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6fe4fb54-3be9-4ce7-9f95-b6d587441143",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVIT_large_gpt2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVAL_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#max_steps=1500, # delete for full training\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#TRAIN_EPOCHS\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:122\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, neftune_noise_alpha, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/training_args.py:1442\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m   1437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1442\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1447\u001b[0m ):\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1449\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1451\u001b[0m     )\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1454\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1463\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/training_args.py:1887\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 1887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/training_args.py:1787\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   1786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available(min_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1787\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   1788\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1789\u001b[0m         )\n\u001b[1;32m   1790\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='VIT_large_gpt2',\n",
    "    per_device_train_batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.VAL_BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1024,  \n",
    "    save_steps=2048, \n",
    "    warmup_steps=1024,  \n",
    "    learning_rate = 5e-5,\n",
    "    #max_steps=1500, # delete for full training\n",
    "    num_train_epochs = config.EPOCHS, #TRAIN_EPOCHS\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f4f64-763b-46e6-9d64-76baef21a30d",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45ba44-7f0f-4ea7-88e8-67ae36d88c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer=feature_extractor,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
