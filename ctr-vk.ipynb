{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DataFrame","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-13T20:05:13.809430Z","iopub.execute_input":"2023-11-13T20:05:13.810163Z","iopub.status.idle":"2023-11-13T20:05:14.332529Z","shell.execute_reply.started":"2023-11-13T20:05:13.810124Z","shell.execute_reply":"2023-11-13T20:05:14.330815Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-11-14T21:47:23.633514Z","iopub.execute_input":"2023-11-14T21:47:23.633935Z","iopub.status.idle":"2023-11-14T21:47:25.920427Z","shell.execute_reply.started":"2023-11-14T21:47:23.633891Z","shell.execute_reply":"2023-11-14T21:47:25.918689Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt', sep=',')","metadata":{"execution":{"iopub.status.busy":"2023-11-14T21:47:25.924570Z","iopub.execute_input":"2023-11-14T21:47:25.925382Z","iopub.status.idle":"2023-11-14T21:47:26.087463Z","shell.execute_reply.started":"2023-11-14T21:47:25.925328Z","shell.execute_reply":"2023-11-14T21:47:26.085923Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import random","metadata":{"execution":{"iopub.status.busy":"2023-11-14T21:47:26.089813Z","iopub.execute_input":"2023-11-14T21:47:26.090730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['ctr'] = pd.Series([random.randint(0, 1) for i in range(len(df))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df.sample(n=int(len(df)*0.8), random_state=42)\ndf_val = df.drop(df_train.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config, device","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Untils","metadata":{}},{"cell_type":"code","source":"import torch\n\n\ndef tokenize(tokenizer, texts, prefix_len=17):\n    input_ids = []\n    attention_masks = []\n    for sent in texts:\n        encoded_dict = tokenizer.encode_plus(\n                            sent,\n                            truncation = True,\n                            add_special_tokens = True,\n#                             max_length = prefix_len,\n                            padding = 'max_length',\n                            max_length = prefix_len,\n                            return_attention_mask = True,\n                            return_tensors = 'pt', \n                      )\n        input_ids.append(encoded_dict['input_ids'])\n    input_ids = torch.cat(input_ids, dim=0)\n    return input_ids\n\n\ndef _convert_image_to_rgb(image):\n    return image.convert(\"RGB\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, GPT2Tokenizer\nimport pandas as pd\nimport os\nfrom PIL import Image\nfrom torchvision import transforms as transforms_\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.data import ConcatDataset\nimport cv2\n# try:\n#     from torchvision.transforms import InterpolationMode\n#     BICUBIC = InterpolationMode.BICUBIC\n# except ImportError:\n#     BICUBIC = Image.BICUBIC","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CTRDataset(Dataset):\n    \n    def __init__(self, dir, df, transform, tokenizer, prefix_len=17):\n        self.df = df\n        self.dir = dir\n        self.prefix_len = prefix_len\n        self.tokenizer = BertTokenizer.from_pretrained(tokenizer)\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n\n        image_name = self.df['image'].iloc[idx]\n        text = self.df['caption'].iloc[idx]\n        \n        input_ids = tokenize(self.tokenizer, [text])\n        input_ids = input_ids[0].to(torch.long)\n        \n        image = cv2.imread(os.path.join(self.dir, image_name))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = Image.fromarray(image)\n        image = self.transform(image)\n        \n        return image.squeeze(), input_ids.to(dtype=torch.float32), df['ctr'].iloc[idx]\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = transforms_.Compose([\n        transforms_.Resize(224, interpolation=BICUBIC),\n        transforms_.CenterCrop(224),\n        _convert_image_to_rgb,\n        transforms_.ToTensor(),\n        transforms_.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n])\n\ntransform_aug = transforms_.Compose([\n    transforms_.Resize((224, 224)),\n    transforms_.RandomHorizontalFlip(), # Пример аугментации\n    transforms_.ToTensor(),\n    transforms_.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_norm = CTRDataset('/kaggle/input/flickr8kimagescaptions/flickr8k/images', df_train, transforms, 'bert-base-multilingual-cased')\ntrain_dataset_aug = CTRDataset('/kaggle/input/flickr8kimagescaptions/flickr8k/images', df_train, transform_aug, 'bert-base-multilingual-cased')\ntrain_dataset = train_dataset_norm + train_dataset_aug\nval_dataset = CTRDataset('/kaggle/input/flickr8kimagescaptions/flickr8k/images', df_val, transforms, 'bert-base-multilingual-cased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_norm[0][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset=train_dataset,\n                          batch_size=200,\n                          shuffle=True,\n                          pin_memory=True,\n                          num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loader = DataLoader(dataset=train_dataset,\n                          batch_size=200,\n                          shuffle=False,\n                          pin_memory=True,\n                          num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass CTRModel(nn.Module):\n    def __init__(self, image_input_size, text_input_size, hidden_size, output_size, num_layers=1):\n        super(CTRModel, self).__init__()\n        \n        self.image_embedding = nn.Linear(image_input_size, hidden_size)\n        self.text_embedding = nn.Linear(text_input_size, hidden_size)\n\n        self.rnn = nn.RNN(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n\n        self.output_layer = nn.Linear(hidden_size, output_size)\n\n    def forward(self, image_vector, text_vector):\n        image_embedding = self.image_embedding(image_vector)\n        text_embedding = self.text_embedding(text_vector)\n        \n        image_embedding = image_embedding.view(image_embedding.size(0), -1)\n        text_embedding = text_embedding.view(text_embedding.size(0), -1)\n        \n        print(image_embedding.size())\n        print(text_embedding.size())\n\n        combined_embedding = torch.cat((text_embedding, image_embedding), dim=-1)\n\n        rnn_output, _ = self.rnn(combined_embedding)\n\n        last_rnn_output = rnn_output[:, -1, :]\n        \n        output = self.output_layer(last_rnn_output)\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import timm\n# timm.list_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\n\nclass CTRModel(nn.Module):\n    def __init__(self, text_size, embed_dim, num_classes):\n        super(CTRModel, self).__init__()\n\n        self.embedding = nn.Linear(text_size, embed_dim)\n        self.rnn = nn.LSTM(embed_dim, 128, batch_first=True)\n\n        self.backbone = timm.create_model('vit_base_patch8_224', pretrained=True)\n        self.backbone.head = nn.Linear(self.backbone.head.in_features, 128)\n\n        self.fc = nn.Linear(256, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, image, text):\n        print(text.dtype)\n        print(text)\n        text = self.embedding(text)\n        _, (text, _) = self.rnn(text)\n        text = text[-1]\n\n        image = self.backbone(image)\n\n        combined = torch.cat((text, image), dim=1)\n        combined = self.dropout(combined)\n\n        output = self.fc(combined)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, loss_fn, optimizer, scheduler, epochs_num=1, device='cuda', verbose=10):\n    model.to(device)\n    \n    for epoch in range(1, epochs_num + 1):\n        model.train()\n        total_loss = 0.0\n\n        for batch_idx, (image, text_vector, ctr) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch}', disable=not verbose)):\n            image, text_vector, ctr = image.to(device), text_vector.to(device), ctr.to(device)\n\n            optimizer.zero_grad()\n            output = model(image, text_vector)\n            loss = loss_fn(output, ctr)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n            if batch_idx % verbose == 0 and batch_idx > 0:\n                avg_loss = total_loss / verbose\n                print(f'Epoch {epoch}, Batch {batch_idx}, Avg. Loss: {avg_loss:.4f}')\n                total_loss = 0.0\n\n        if scheduler is not None:\n            scheduler.step()\n\n        if val_loader is not None:\n            model.eval()\n            val_loss = 0.0\n            with torch.no_grad():\n                for val_image, val_text_vector, val_ctr in tqdm(val_loader, desc='Validation', disable=not verbose):\n                    val_image, val_text_vector, val_ctr = val_image.to(device), val_text_vector.to(device), val_ctr.to(device)\n                    val_output = model(val_image, val_text_vector)\n                    val_loss += loss_fn(val_output, val_ctr).item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            print(f'Epoch {epoch}, Validation Loss: {avg_val_loss:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CTRModel(77, 100, 1)\n\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(model, train_loader, val_loader, loss_function, optimizer, scheduler, epochs_num=1, device=device, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass YourModel(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(YourModel, self).__init__()\n\n        # Добавление линейного слоя\n        self.linear_layer = nn.Linear(input_size, output_size)\n\n    def forward(self, text_vector):\n        # Применение линейного слоя к входному текстовому вектору\n        output = self.linear_layer(text_vector)\n\n        return output\n\n# Пример использования\ninput_size = 77  # Пример: размерность вектора текста (BERT эмбеддинг)\noutput_size = 37  # Пример: размерность выходного вектора\n\nmodel = YourModel(input_size, output_size)\n\n# Пример входного текстового вектора\ntext_vector = train_dataset[1][1]  # 32 - размер пакета (batch_size)\n\n# Применение модели к входному текстовому вектору\noutput = model(text_vector)\n\n# Вывод размерности выходного вектора\nprint(\"Размер выходного вектора:\", output.size())","metadata":{"execution":{"iopub.status.busy":"2023-11-13T22:00:52.532001Z","iopub.execute_input":"2023-11-13T22:00:52.532534Z","iopub.status.idle":"2023-11-13T22:00:52.721186Z","shell.execute_reply.started":"2023-11-13T22:00:52.532474Z","shell.execute_reply":"2023-11-13T22:00:52.719892Z"},"trusted":true},"execution_count":148,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[148], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m text_vector \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# 32 - размер пакета (batch_size)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Применение модели к входному текстовому вектору\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Вывод размерности выходного вектора\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mРазмер выходного вектора:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39msize())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[148], line 13\u001b[0m, in \u001b[0;36mYourModel.forward\u001b[0;34m(self, text_vector)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_vector):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Применение линейного слоя к входному текстовому вектору\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"],"ename":"RuntimeError","evalue":"expected scalar type Long but found Float","output_type":"error"}]},{"cell_type":"code","source":"model = timm.create_model('resnet50', pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:40:19.195439Z","iopub.execute_input":"2023-11-14T18:40:19.195877Z","iopub.status.idle":"2023-11-14T18:40:20.440423Z","shell.execute_reply.started":"2023-11-14T18:40:19.195844Z","shell.execute_reply":"2023-11-14T18:40:20.439346Z"},"trusted":true},"execution_count":65,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1b1509cf8bb43d39aa4d55772334834"}},"metadata":{}}]},{"cell_type":"code","source":"model.head = nn.Linear(140, 128)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:40:20.442428Z","iopub.execute_input":"2023-11-14T18:40:20.442802Z","iopub.status.idle":"2023-11-14T18:40:20.447922Z","shell.execute_reply.started":"2023-11-14T18:40:20.442773Z","shell.execute_reply":"2023-11-14T18:40:20.446813Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:40:20.449529Z","iopub.execute_input":"2023-11-14T18:40:20.450193Z","iopub.status.idle":"2023-11-14T18:40:20.464491Z","shell.execute_reply.started":"2023-11-14T18:40:20.450142Z","shell.execute_reply":"2023-11-14T18:40:20.463410Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act1): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n  (head): Linear(in_features=140, out_features=128, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# ChatGPT","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models import resnet50\nimport pandas as pd\nfrom PIL import Image\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchtext.vocab import Vocab\n\n# Предположим, что у вас есть DataFrame с именами столбцов 'text', 'image_path', 'target'\ndf = pd.read_csv('your_dataset.csv')\n\n# Предобработка текста\ndef preprocess_text(text):\n    # Здесь должна быть ваша логика предобработки текста, например:\n    tokens = word_tokenize(text.lower())\n    return tokens\n\n# Подсчет частоты слов для создания словаря\ncounter = Counter()\nfor text in df['text']:\n    counter.update(preprocess_text(text))\n\nvocab = Vocab(counter, min_freq=1)\n\n# Предобработка изображений\nimage_transform = transforms.Compose([\ntransforms.Resize((224, 224)),\ntransforms.ToTensor(),\n])\n\n# Создание датасета для PyTorch\nclass AdvertisementDataset(Dataset):\n    def __init__(self, dataframe, vocab, image_transform):\n        self.dataframe = dataframe\n        self.vocab = vocab\n        self.image_transform = image_transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        text = torch.tensor([self.vocab.stoi[token] for token in preprocess_text(row['text'])], dtype=torch.long)\n        image = Image.open(row['image_path']).convert('RGB')\n        image = self.image_transform(image)\n        target = torch.tensor(row['target'], dtype=torch.float)\n            return text, image, target\n\ndataset = AdvertisementDataset(df, vocab, image_transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Определение модели\nclass MultimodalNN(nn.Module):\n    \n    def __init__(self, vocab_size, embed_dim, num_classes):\n        super(MultimodalNN, self).__init__()\n    \n        # Текстовая часть\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn = nn.LSTM(embed_dim, 128, batch_first=True)\n        # Изображение часть\n        self.resnet = resnet50(pretrained=True)\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 128)\n        \n        for param in self.resnet.parameters():\n            param.requires_grad = True\n            \n        # Объединение\n        self.fc = nn.Linear(256, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, text, image):\n        text = self.embedding(text)\n        _, (text, _) = self.rnn(text)\n        text = text[-1]\n        image = self.resnet(image)\n        combined = torch.cat((text, image), dim=1)\n        combined = self.dropout(combined)\n        output = self.fc(combined)\n        \n        return output\n\nmodel = MultimodalNN(len(vocab), embed_dim=100, num_classes=1)\n\n# Обучение модели\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for texts, images, targets in dataloader:\n        optimizer.zero_grad()\n        outputs = model(texts, images)\n        loss = criterion(outputs.squeeze(), targets)\n        loss.backward()\n        optimizer.step()\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Links","metadata":{}},{"cell_type":"markdown","source":"MultiModel - https://drivendata.co/blog/hateful-memes-benchmark/ <br>\nTextClassifier - https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html <br>\ntorch_videovision - https://github.com/hassony2/torch_videovision <br>\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}